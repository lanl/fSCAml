{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ac4751-76f4-4a70-af12-af353a827988",
   "metadata": {},
   "source": [
    "# Train gradient boosted trees ML for fSCA prediction\n",
    "\n",
    "- Purpose: Train a gradient boosted trees ML model for fractional snow covered area (fSCA) using GPUs\n",
    "- Creator: Cade Trotter with input from Ryan Crumley and Katrina Bennett\n",
    "- Created: 2025.06.09; Last Modification: 2025.10.14\n",
    "- See also: chunkedML.py\n",
    "- Note: we used AI to create portions of the code below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09ff44-9c82-41be-83fb-1ce27b94d5a3",
   "metadata": {},
   "source": [
    "### Import packages / libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372f029-c4df-4a7f-a6dc-acf70eb6c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import dask\n",
    "import dask.array as da\n",
    "import json\n",
    "\n",
    "dask.config.set(array__slicing__split_large_chunks=True)\n",
    "\n",
    "print(\"All modules imported successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acf98f-adec-4105-a412-4b5777568f06",
   "metadata": {},
   "source": [
    "## 1) User inputs and data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a381b-8fc6-4ca9-909c-ef7933c9160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these paths for your purposes, the remainder of the code should not need to be changed\n",
    "\n",
    "trainingWaterYears = [2011, 2012, 2013, 2015, 2016, 2017, 2018] #saving 2010, 2014, 2019 for validation. Oct 1 of WY -1 : Sep 30 of WY\n",
    "n_chunks = len(trainingWaterYears) * 10 ### this sets how many chunks we will make. More chunks = less GPU memory at a given time\n",
    "env = os.environ.copy()\n",
    "env[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Or \"\" for all GPUs. This specifies which GPU to use\n",
    "\n",
    "### Base Path:\n",
    "dataDirectoryPath = ###put your base dir path here. If you do not wish to follow the dir structure below, paths will need to be\n",
    "### ammended to reflect that. \n",
    "\n",
    "### Model Path:\n",
    "base_run_path = dataDirectoryPath + \"ML_model_dir/ML_model_version/\" ###adjust depending on what you are doing\n",
    "\n",
    "### Output paths\n",
    "model_base = base_run_path + \"model/\" ###where the model will be stored during training and once done. Make this dir\n",
    "npz_base = base_run_path +  \"chunks/\" ###where the IID training data chunks will be stored during training and once done. Make this dir\n",
    "model_features_base = base_run_path + \"featuresJsons/\" ###where the features used in training will be stored during training and once done. Make this dir\n",
    "\n",
    "### Save Paths (these should not need to be changed)\n",
    "model_path = \"model.pkl\"\n",
    "chunk_path = \"chunk\"\n",
    "features_path = \"feature_names.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1d172-bdee-49f7-852e-28d01b2139d3",
   "metadata": {},
   "source": [
    "### Establish File Paths to Important Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c0271-9a12-497a-a0bb-0a080e52be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: If you do not wish to follow the dir structure below, paths will need to be\n",
    "### ammended to reflect that. \n",
    "\n",
    "### The dims in these datasets should be in two categories:\n",
    "### Static: (x,y)\n",
    "### Dynamic: (time, x, y)\n",
    "### Since they are being combined by coords in xarray the (time), x and y need to match in shape and value. Consider this in data prep\n",
    "\n",
    "### Topographic (x,y):\n",
    "elevPath = \"goodDemXY.nc\"\n",
    "slopeAspectPath = \"slopeAspectBinnedXY.nc\"\n",
    "latLongPath = \"latLongXY.nc\"\n",
    "\n",
    "### Unique (time x,y):\n",
    "dayOfWaterYearPaths = [dataDirectoryPath + \"dowy/dayOfWaterYearGriddedSurface\" + str(waterYear) + \".nc\" for waterYear in trainingWaterYears]\n",
    "\n",
    "### Forcing Variables (time x,y):\n",
    "conusPaths = [dataDirectoryPath + \"conusDaily/conusData\" + str(waterYear) + \"Complete.nc\" for waterYear in trainingWaterYears]\n",
    "\n",
    "### FSCA 5cm Threshold for a WY (time x,y)\n",
    "fscaPaths = [dataDirectoryPath + \"smFsca/smFsca_regridded_\" + str(waterYear-1) + str(waterYear) + \"XY.nc\" for waterYear in trainingWaterYears]\n",
    "\n",
    "### Precipitation Cummulative Summation (time x,y) (derived from CONUS: tiled, cummulative sum of RAINRATE starting on each water year)\n",
    "precipPaths = [dataDirectoryPath + \"conusDaily/conusData\" + str(waterYear) + \"Complete_cumsum_of_RAINRATE.nc\" for waterYear in trainingWaterYears]\n",
    "\n",
    "### Static Paths such as elev, slope, etc:\n",
    "staticList = [elevPath, slopeAspectPath, latLongPath]\n",
    "staticPaths = [dataDirectoryPath + \"topo/\" + path for path in staticList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ade8b-7885-40a3-99ef-98074119d096",
   "metadata": {},
   "source": [
    "### Inspect the paths (if desired, uncomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35356e06-6d64-4cf5-936f-ca46f0cb9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# staticPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebf37b-90a6-4c86-b9d1-6939a9cf3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conusPaths + fscaPaths + dayOfWaterYearPaths + precipPaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c45a96-1ae4-42c0-8062-10b57b4135af",
   "metadata": {},
   "source": [
    "### Establish the dynamic and static predictor variables, and the target variable for the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cd249-5f1c-4950-9588-dae577a17086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dynamic or static predictor variables\n",
    "\n",
    "# The dynamic variables are those that change with each timestep\n",
    "dynamic_vars = ['T2D', 'LWDOWN', 'SWDOWN', 'U2D', 'V2D', 'day_of_year', 'accumulated_RAINRATE']    # Must be (time, x, y)\n",
    "\n",
    "# The static variables are those that remain the same with each timestep (usually physiographic or geographic)\n",
    "static_vars = ['HGT', 'slope', \"binned_aspect\", \"lat\", \"lon\"]   # Must be (x, y)\n",
    "\n",
    "# The target variable\n",
    "target_var = 'fSCA' # Must be (time, x, y)\n",
    "\n",
    "# Load Datasets Lazily \n",
    "ds_dyn = xr.open_mfdataset(conusPaths + fscaPaths + dayOfWaterYearPaths + precipPaths, combine='by_coords', parallel=True, chunks={'time': 100})\n",
    "ds_static = xr.open_mfdataset(staticPaths, combine='by_coords', parallel=True)  # elevation, slope etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de06f2-f15d-485d-845b-289aa6512c53",
   "metadata": {},
   "source": [
    "### Fix some coords issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ad7be-afb6-439c-96a6-bafe3d6d6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dyn = ds_dyn.reset_coords([\"lat\", \"lon\"], drop=False)\n",
    "ds_dyn['lat'] = ds_dyn['lat'].transpose(\"x\", \"y\")\n",
    "ds_dyn['lon'] = ds_dyn['lon'].transpose(\"x\", \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59722e8d-538c-44df-a900-67318193c1fb",
   "metadata": {},
   "source": [
    "### Expand Dims and Combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28048ef9-e4bb-4961-9194-80842d9838bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand static variables across time \n",
    "ds_static_exp = ds_static.expand_dims(time=ds_dyn.time)\n",
    "\n",
    "# Merge dynamic + static + target \n",
    "ds_combined = xr.merge([ds_dyn[dynamic_vars + [target_var]], ds_static_exp[static_vars]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e08f00-638f-4658-92e3-ec1d9031bdcf",
   "metadata": {},
   "source": [
    "## 2) Reshape predictor variables and target variables for the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cfc6a-ca20-4b17-b040-e23378e14645",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = ds_combined[dynamic_vars + static_vars].to_array().transpose(...).data.astype(np.float32)\n",
    "y_all = ds_combined[target_var].data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030a618-a841-4bfa-bcf3-ff87d9063465",
   "metadata": {},
   "source": [
    "### Keep the variable labels for importance metrics in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010036c-6185-4703-9d7a-0055f6751cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(ds_combined[dynamic_vars + static_vars].to_array().coords['variable'].values)\n",
    "\n",
    "with open(model_features_base + features_path, \"w\") as f:\n",
    "    json.dump([str(f) for f in feature_names], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d883fde-4daa-49e6-afb7-237415ecfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples:\", X_all.shape)\n",
    "print(\"Total samples:\", y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e1cbb-0837-4039-95da-5939ada6a2de",
   "metadata": {},
   "source": [
    "### Transpose and flatten data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b66a77-83ef-4832-a121-a504b74d93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose X_all from (vars, time, x, y) â†’ (time, x, y, vars)\n",
    "X_all = X_all.transpose(1, 2, 3, 0)\n",
    "\n",
    "# Transpose y_all to match X_all's spatial layout\n",
    "y_all = y_all.transpose(0, 2, 1) \n",
    "\n",
    "# Flatten both\n",
    "X_all = X_all.reshape(-1, 12)  \n",
    "y_all = y_all.reshape(-1)       \n",
    "\n",
    "#fix issue with NaNs in target data:\n",
    "# expect that this will take a good while as the data is going from a lazy format to more eager when we compute and move it to Numpy format\n",
    "mask = da.isfinite(y_all).compute()  # make the mask NumPy\n",
    "X_all = X_all.compute()[mask]\n",
    "y_all = y_all.compute()[mask]\n",
    "\n",
    "###here we are expecting the data to be of shape (time*x*y, vars)\n",
    "print(\"shape(X_all): \", np.shape(X_all))\n",
    "print(\"shape(y_all): \", np.shape(y_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cca8f-07f9-40da-a46e-71875295c4c4",
   "metadata": {},
   "source": [
    "## 3) Make the data I.I.D. (random) and establish predictors & target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52111d8-73c5-46fd-a502-111d7ca2a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independently and identically distributed = IID\n",
    "\n",
    "# Create a list of indices that are random\n",
    "n_samples = X_all.shape[0]\n",
    "chunk_size = n_samples // n_chunks\n",
    "\n",
    "# Reproducible shuffle\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Distribute random indices across chunks equally\n",
    "chunk_indices = np.array_split(shuffled_indices, n_chunks)\n",
    "\n",
    "# Define feature and target variables\n",
    "predictor_vars = dynamic_vars + static_vars\n",
    "target_var = target_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a40962-75da-4e50-9997-4861cdb58d7f",
   "metadata": {},
   "source": [
    "## 4) Train using chunkedML.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29338301-c1f4-4519-b324-066183b14771",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we made a set of chunks to reduce GPU memory used at any given time. We are iterating over them here and training the model with them\n",
    "for i, idx in enumerate(chunk_indices):\n",
    "    print(f\" Processing chunk {i}...\")\n",
    "\n",
    "    ### quick check\n",
    "    if len(idx) == 0:\n",
    "        print(f\"Skipping chunk {i}: no samples\")\n",
    "        continue\n",
    "    \n",
    "    # proceed with training...\n",
    "    x_path = f\"{npz_base + chunk_path}_{i}_X.npy\"\n",
    "    y_path = f\"{npz_base + chunk_path}_{i}_y.npy\"\n",
    "\n",
    "    if os.path.exists(x_path) and os.path.exists(y_path):\n",
    "        print(f\" Chunk {i} already saved â€” skipping recomputation.\")\n",
    "    else:\n",
    "        idx_sorted = np.sort(idx)\n",
    "        inv = np.argsort(np.argsort(idx))\n",
    "    \n",
    "        X_sorted = X_all[idx_sorted]\n",
    "        y_sorted = y_all[idx_sorted]\n",
    "    \n",
    "        # Restore original shuffle order\n",
    "        X_chunk = X_sorted[inv].reshape(-1, X_all.shape[1])  # shape (n_samples, n_features)\n",
    "        y_chunk = y_sorted[inv].reshape(-1)  # shape (n_samples,)\n",
    "    \n",
    "        # Save to .npy files\n",
    "        np.save(x_path, X_chunk)\n",
    "        np.save(y_path, y_chunk)\n",
    "        print(f\"ðŸ’¾ Saved chunk {i} to disk.\")\n",
    "\n",
    "    # Build command to overwrite the same model file\n",
    "    cmd = [\n",
    "        \"python\", \"/path/to/python/script/chunkedML.py\",\n",
    "        \"--X\", x_path,\n",
    "        \"--y\", y_path,\n",
    "        \"--model-out\", model_base + model_path,\n",
    "        \"--n-estimators\", \"50\",\n",
    "        \"--device\", \"gpu\",\n",
    "        \"--feature_names\", model_features_base + features_path\n",
    "    ]\n",
    "\n",
    "    if os.path.exists(model_base + model_path):\n",
    "        cmd += [\"--model-in\", model_base + model_path]\n",
    "\n",
    "    print(f\" Training model on chunk {i}...\")\n",
    "    result = subprocess.run(cmd, env=env, text=True)\n",
    "\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
